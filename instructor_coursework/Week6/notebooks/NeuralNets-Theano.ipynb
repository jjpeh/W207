{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Classification with Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interest in neural networks, and in particular those with architechures that support deep learning, has been surging in recent years.  \n",
    "\n",
    "In this notebook we will be revisiting the problem of digit classification on the MNIST data.  In doing so, we will introduce a new Python library, Theano, for working with neural networks.  Theano is a popular choice for neural networks as the same code can be run on either CPUs or GPUs.  GPUs greatly speed up the training and prediction, and is readily available. Amazon even offers GPU machines on EC2.  \n",
    "\n",
    "In part 1, we'll introduce Theano, and refresh ourselves on the MNIST dataset.  In part 2, we'll create a multi-layer neural network with a simple architechure, and train it using backpropagation.  Part 3 will introduce the convolutional architechure, which can be said to be doing 'deep learning' (also called feature learning or representation learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start to look at Theano.  If later you'd like to go deeper into Theano, you may want to read this paper: http://www.iro.umontreal.ca/~lisa/pointeurs/theano_scipy2010.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Theano if you haven't already.  Then let's load it, and set it to work with a CPU.  For reference, here is the Theano documentation: http://www.deeplearning.net/software/theano/library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "\n",
    "import theano \n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "print (theano.config.device) # We're using CPUs (for now)\n",
    "print (theano.config.floatX) # Should be 64 bit for CPUs\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for MNIST data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features = 784\n",
      "Train set = 2000\n",
      "Test set = 2000\n"
     ]
    }
   ],
   "source": [
    "# Repeating steps from Project 1 to prepare mnist dataset. \n",
    "mnist = fetch_mldata('MNIST original', data_home='~/datasets/mnist')\n",
    "X, Y = mnist.data, mnist.target\n",
    "X = X / 255.0\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "numExamples = 2000\n",
    "test_data, test_labels = X[70000-numExamples:], Y[70000-numExamples:]\n",
    "train_data, train_labels = X[:numExamples], Y[:numExamples]\n",
    "numFeatures = train_data[1].size\n",
    "numTrainExamples = train_data.shape[0]\n",
    "numTestExamples = test_data.shape[0]\n",
    "print ('Features = %d' %(numFeatures))\n",
    "print ('Train set = %d' %(numTrainExamples))\n",
    "print ('Test set = %d' %(numTestExamples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking ahead to working with neural networks, let's prepare one additional variation of the label data.  Let's make these labels, rather than each being an integer value from 0-9, be a set of 10 binary values, one for each class.  This is sometimes called a 1-of-n encoding, and it makes working with Neural Networks easier, as there will be one output node for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes = 10\n"
     ]
    }
   ],
   "source": [
    "def binarizeY(data):\n",
    "    binarized_data = np.zeros((data.size,10))\n",
    "    for j in range(0,data.size):\n",
    "        feature = data[j:j+1]\n",
    "        i = feature.astype(np.int64) \n",
    "        binarized_data[j,i]=1\n",
    "    return binarized_data\n",
    "train_labels_b = binarizeY(train_labels)\n",
    "test_labels_b = binarizeY(test_labels)\n",
    "numClasses = train_labels_b[1].size\n",
    "print ('Classes = %d' %(numClasses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with a KNN model to establish a baseline accuracy.\n",
    "\n",
    "Exercise: You've seen a number of different classification algorithms (e.g. naive bayes, decision trees, random forests, logistic regression) at this point.  How does KNN scalability and accuracy with respect to the size of the training dataset compare to those other algorithms?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time = 0.04\n",
      "Accuracy = 0.9130\n",
      "Prediction time = 4.82\n"
     ]
    }
   ],
   "source": [
    "neighbors = 1\n",
    "knn = KNeighborsClassifier(neighbors)\n",
    "# we'll be waiting quite a while if we use 60K examples, so let's cut it down.  You may want to run the full 60K on your own later to see what the accuracy is.\n",
    "mini_train_data, mini_train_labels = X[:numExamples], Y[:numExamples] \n",
    "start_time = time.time()\n",
    "knn.fit(mini_train_data, mini_train_labels)\n",
    "print ('Train time = %.2f' %(time.time() - start_time))\n",
    "start_time = time.time()\n",
    "accuracy = knn.score(test_data, test_labels)\n",
    "print ('Accuracy = %.4f' %(accuracy))\n",
    "print ('Prediction time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now that we have a simple baseline, let's start working in Theano.  Before we jump to multi-layer neural networks though, let's train a logistic regression model to make certain we're using Theano correctly. \n",
    "\n",
    "Recall from Josh's regression lecture the four key components: (1) parameters, (2) model, (3) cost function, and (4) objective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (1) Parameters \n",
    "# Initialize the weights to small, but non-zero, values.\n",
    "w = theano.shared(np.asarray((np.random.randn(*(numFeatures, numClasses))*.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two notes relevant at this point:\n",
    "\n",
    "First, logistic regression can be thought of as a neural network with no hidden layers. The output values are just the dot product of the inputs and the edge weights.\n",
    "\n",
    "Second, we have 10 classes. We can either train separate one vs all classifiers using sigmoid activation, which would be a hassle, or we can use the softmax activation, which is essentially a multi-class version of sigmoid. We'll use Theano's built-in implementation of softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (2) Model\n",
    "# Theano objects accessed with standard Python variables\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "\n",
    "def model(X, w):\n",
    "    return T.nnet.softmax(T.dot(X, w))\n",
    "y_hat = model(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use cross-entropy as a cost function.  Cross entropy only considers the error between the true class and the prediction, and not the errors for the false classes.  This tends to cause the network to converge faster.  We'll use Theano's built-in cross entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (3) Cost function\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is minimize the cost, and to do that we'll use batch gradient descent.\n",
    "\n",
    "Exercise: What are the differences between batch, stochastic, and mini-batch gradient descent?  What are the implications of each for working on large datasets?\n",
    "\n",
    "We'll use Theano's built-in gradient function.  Exercise: Do you recall from Josh's lecture what the gradient is for beta in logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) accuracy = 0.1085\n",
      "2) accuracy = 0.1370\n",
      "3) accuracy = 0.1685\n",
      "4) accuracy = 0.1955\n",
      "5) accuracy = 0.2235\n",
      "6) accuracy = 0.2565\n",
      "7) accuracy = 0.2900\n",
      "8) accuracy = 0.3265\n",
      "9) accuracy = 0.3650\n",
      "10) accuracy = 0.3980\n",
      "11) accuracy = 0.4320\n",
      "12) accuracy = 0.4620\n",
      "13) accuracy = 0.4875\n",
      "14) accuracy = 0.5130\n",
      "15) accuracy = 0.5395\n",
      "16) accuracy = 0.5610\n",
      "17) accuracy = 0.5785\n",
      "18) accuracy = 0.5910\n",
      "19) accuracy = 0.6045\n",
      "20) accuracy = 0.6090\n",
      "21) accuracy = 0.6210\n",
      "22) accuracy = 0.6265\n",
      "23) accuracy = 0.6365\n",
      "24) accuracy = 0.6445\n",
      "25) accuracy = 0.6505\n",
      "26) accuracy = 0.6555\n",
      "27) accuracy = 0.6580\n",
      "28) accuracy = 0.6625\n",
      "29) accuracy = 0.6690\n",
      "30) accuracy = 0.6730\n",
      "31) accuracy = 0.6765\n",
      "32) accuracy = 0.6810\n",
      "33) accuracy = 0.6890\n",
      "34) accuracy = 0.6915\n",
      "35) accuracy = 0.6955\n",
      "36) accuracy = 0.6970\n",
      "37) accuracy = 0.7000\n",
      "38) accuracy = 0.7030\n",
      "39) accuracy = 0.7060\n",
      "40) accuracy = 0.7075\n",
      "41) accuracy = 0.7110\n",
      "42) accuracy = 0.7115\n",
      "43) accuracy = 0.7130\n",
      "44) accuracy = 0.7145\n",
      "45) accuracy = 0.7175\n",
      "46) accuracy = 0.7210\n",
      "47) accuracy = 0.7235\n",
      "48) accuracy = 0.7260\n",
      "49) accuracy = 0.7255\n",
      "50) accuracy = 0.7265\n",
      "train time = 0.26\n",
      "predict time = 0.00\n"
     ]
    }
   ],
   "source": [
    "## (4) Objective (and solver)\n",
    "\n",
    "alpha = 0.01\n",
    "gradient = T.grad(cost=cost, wrt=w) \n",
    "update = [[w, w - gradient * alpha]] \n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True) # computes cost, then runs update\n",
    "y_pred = T.argmax(y_hat, axis=1) # select largest probability as prediction\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "def gradientDescent(epochs):\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    for i in range(epochs):\n",
    "        start_time = time.time()\n",
    "        cost = train(train_data[0:len(train_data)], train_labels_b[0:len(train_data)])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print ('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data))))\n",
    "    print ('train time = %.2f' %(trainTime))\n",
    "\n",
    "gradientDescent(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(test_data)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:  What do you expect to happen if we convert batch gradient descent to stochastic gradient descent?  Why?\n",
    "\n",
    "Let's try it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) accuracy = 0.8445\n",
      "2) accuracy = 0.8640\n",
      "3) accuracy = 0.8710\n",
      "4) accuracy = 0.8720\n",
      "5) accuracy = 0.8705\n",
      "6) accuracy = 0.8695\n",
      "7) accuracy = 0.8695\n",
      "8) accuracy = 0.8685\n",
      "9) accuracy = 0.8705\n",
      "10) accuracy = 0.8715\n",
      "11) accuracy = 0.8720\n",
      "12) accuracy = 0.8710\n",
      "13) accuracy = 0.8710\n",
      "14) accuracy = 0.8720\n",
      "15) accuracy = 0.8705\n",
      "16) accuracy = 0.8700\n",
      "17) accuracy = 0.8705\n",
      "18) accuracy = 0.8725\n",
      "19) accuracy = 0.8730\n",
      "20) accuracy = 0.8730\n",
      "21) accuracy = 0.8730\n",
      "22) accuracy = 0.8725\n",
      "23) accuracy = 0.8725\n",
      "24) accuracy = 0.8735\n",
      "25) accuracy = 0.8740\n",
      "26) accuracy = 0.8735\n",
      "27) accuracy = 0.8740\n",
      "28) accuracy = 0.8735\n",
      "29) accuracy = 0.8735\n",
      "30) accuracy = 0.8730\n",
      "31) accuracy = 0.8720\n",
      "32) accuracy = 0.8720\n",
      "33) accuracy = 0.8720\n",
      "34) accuracy = 0.8720\n",
      "35) accuracy = 0.8720\n",
      "36) accuracy = 0.8720\n",
      "37) accuracy = 0.8725\n",
      "38) accuracy = 0.8730\n",
      "39) accuracy = 0.8735\n",
      "40) accuracy = 0.8735\n",
      "41) accuracy = 0.8735\n",
      "42) accuracy = 0.8740\n",
      "43) accuracy = 0.8740\n",
      "44) accuracy = 0.8735\n",
      "45) accuracy = 0.8730\n",
      "46) accuracy = 0.8725\n",
      "47) accuracy = 0.8725\n",
      "48) accuracy = 0.8725\n",
      "49) accuracy = 0.8725\n",
      "50) accuracy = 0.8725\n",
      "train time = 86.92\n",
      "predict time = 0.00\n"
     ]
    }
   ],
   "source": [
    "## (1) Parameters\n",
    "w = theano.shared(np.asarray((np.random.randn(*(numFeatures, numClasses))*.01)))\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "def model(X, w):\n",
    "    return T.nnet.softmax(T.dot(X, w))\n",
    "y_hat = model(X, w)\n",
    "\n",
    "## (3) Cost\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
    "\n",
    "## (4) Objective\n",
    "alpha = 0.01\n",
    "gradient = T.grad(cost=cost, wrt=w)\n",
    "update = [[w, w - gradient * alpha]] \n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True) \n",
    "y_pred = T.argmax(y_hat, axis=1) \n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "miniBatchSize = 1 \n",
    "def gradientDescentStochastic(epochs):\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs):       \n",
    "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
    "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print ('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data)))    ) \n",
    "    print ('train time = %.2f' %(trainTime))\n",
    "    \n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(test_data)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: What do you expect to happen if you switch the batch size to be great than 1 (i.e. mini-batch)?  Why?\n",
    "\n",
    "Try it for a few values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) accuracy = 0.8025\n",
      "2) accuracy = 0.8310\n",
      "3) accuracy = 0.8405\n",
      "4) accuracy = 0.8525\n",
      "5) accuracy = 0.8605\n",
      "6) accuracy = 0.8635\n",
      "7) accuracy = 0.8665\n",
      "8) accuracy = 0.8665\n",
      "9) accuracy = 0.8695\n",
      "10) accuracy = 0.8720\n",
      "11) accuracy = 0.8730\n",
      "12) accuracy = 0.8740\n",
      "13) accuracy = 0.8740\n",
      "14) accuracy = 0.8775\n",
      "15) accuracy = 0.8785\n",
      "16) accuracy = 0.8780\n",
      "17) accuracy = 0.8800\n",
      "18) accuracy = 0.8800\n",
      "19) accuracy = 0.8800\n",
      "20) accuracy = 0.8810\n",
      "21) accuracy = 0.8800\n",
      "22) accuracy = 0.8815\n",
      "23) accuracy = 0.8815\n",
      "24) accuracy = 0.8825\n",
      "25) accuracy = 0.8830\n",
      "26) accuracy = 0.8835\n",
      "27) accuracy = 0.8835\n",
      "28) accuracy = 0.8835\n",
      "29) accuracy = 0.8835\n",
      "30) accuracy = 0.8845\n",
      "31) accuracy = 0.8840\n",
      "32) accuracy = 0.8840\n",
      "33) accuracy = 0.8840\n",
      "34) accuracy = 0.8850\n",
      "35) accuracy = 0.8850\n",
      "36) accuracy = 0.8850\n",
      "37) accuracy = 0.8850\n",
      "38) accuracy = 0.8860\n",
      "39) accuracy = 0.8860\n",
      "40) accuracy = 0.8865\n",
      "41) accuracy = 0.8865\n",
      "42) accuracy = 0.8865\n",
      "43) accuracy = 0.8870\n",
      "44) accuracy = 0.8870\n",
      "45) accuracy = 0.8870\n",
      "46) accuracy = 0.8875\n",
      "47) accuracy = 0.8875\n",
      "48) accuracy = 0.8875\n",
      "49) accuracy = 0.8880\n",
      "50) accuracy = 0.8885\n",
      "train time = 27.86\n",
      "predict time = 0.00\n"
     ]
    }
   ],
   "source": [
    "## (1) Parameters\n",
    "w = theano.shared(np.asarray((np.random.randn(*(numFeatures, numClasses))*.01)))\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "def model(X, w):\n",
    "    return T.nnet.softmax(T.dot(X, w))\n",
    "y_hat = model(X, w)\n",
    "\n",
    "## (3) Cost\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
    "\n",
    "## (4) Objective\n",
    "alpha = 0.01\n",
    "gradient = T.grad(cost=cost, wrt=w)\n",
    "update = [[w, w - gradient * alpha]] \n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True) \n",
    "y_pred = T.argmax(y_hat, axis=1) \n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "miniBatchSize = 10 \n",
    "def gradientDescentStochastic(epochs):\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs):       \n",
    "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
    "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print ('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data))))     \n",
    "    print ('train time = %.2f' %(trainTime))\n",
    "    \n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(test_data)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
